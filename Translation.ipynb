{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOtTAiT20+5F8FTOS/iEHxx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install torchtext==0.6\n","import torchtext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iv5yXd0Cddew","executionInfo":{"status":"ok","timestamp":1686907781106,"user_tz":-420,"elapsed":6716,"user":{"displayName":"SƠN TRƯƠNG HÀ","userId":"08876576014997023636"}},"outputId":"e5247af6-7117-4329-ab1e-d39571532b26"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchtext==0.6\n","  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (4.65.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (2.27.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (2.0.1+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (1.22.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (1.16.0)\n","Collecting sentencepiece (from torchtext==0.6)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (3.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6) (1.3.0)\n","Installing collected packages: sentencepiece, torchtext\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.15.2\n","    Uninstalling torchtext-0.15.2:\n","      Successfully uninstalled torchtext-0.15.2\n","Successfully installed sentencepiece-0.1.99 torchtext-0.6.0\n"]}]},{"cell_type":"markdown","source":["**Preparing the Data**\n","\n","As always, let's import all the required modules and set the random seeds for reproducability."],"metadata":{"id":"Cv5QlY22qBLr"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import spacy\n","import torchtext\n","from torchtext.datasets import Multi30k\n","from torchtext.data import Field, BucketIterator\n","import numpy as np\n","import random\n","import math\n","import time\n","from torch.utils.tensorboard import SummaryWriter"],"metadata":{"id":"v59VRKiBdfNg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy.cli\n","spacy.cli.download(\"en_core_web_sm\")\n","spacy.cli.download(\"de_core_news_sm\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RUpQK93FiB7X","executionInfo":{"status":"ok","timestamp":1686907834223,"user_tz":-420,"elapsed":9874,"user":{"displayName":"SƠN TRƯƠNG HÀ","userId":"08876576014997023636"}},"outputId":"20a84fdd-fbd8-46a5-ac8d-25b7b4e0a611"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('de_core_news_sm')\n"]}]},{"cell_type":"markdown","source":["We'll set the random seeds for deterministic results."],"metadata":{"id":"RYj660mKqad6"}},{"cell_type":"code","source":["SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"metadata":{"id":"-fJ-ufwD-6r4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we'll create the tokenizers. A tokenizer is used to turn a string containing a sentence into a list of individual tokens that make up that string, e.g. \"good morning!\" becomes [\"good\", \"morning\", \"!\"]. We'll start talking about the sentences being a sequence of tokens from now, instead of saying they're a sequence of words. What's the difference? Well, \"good\" and \"morning\" are both words and tokens, but \"!\" is a token, not a word.\n","\n","spaCy has model for each language (\"de_core_news_sm\" for German and \"en_core_web_sm\" for English) which need to be loaded so we can access the tokenizer of each model.\n","\n","Note: the models must first be downloaded using the following on the command line:\n","\n","python -m spacy download en_core_web_sm\n","python -m spacy download de_core_news_sm\n","We load the models as such:"],"metadata":{"id":"EjK1ysxtqJ99"}},{"cell_type":"code","source":["spacy_de = spacy.load('de_core_news_sm')\n","spacy_en = spacy.load('en_core_web_sm')"],"metadata":{"id":"H7E5x53QiXFH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we create the tokenizer functions. These can be passed to torchtext and will take in the sentence as a string and return the sentence as a list of tokens.\n","\n","In the paper we are implementing, they find it beneficial to reverse the order of the input which they believe \"introduces many short term dependencies in the data that make the optimization problem much easier\". We copy this by reversing the German sentence after it has been transformed into a list of tokens"],"metadata":{"id":"r5-z01OEqnuj"}},{"cell_type":"code","source":["def tokenize_de(text):\n","    \"\"\"\n","    Tokenizes German text from a string into a list of strings\n","    \"\"\"\n","    return [tok.text for tok in spacy_de.tokenizer(text)]\n","\n","def tokenize_en(text):\n","    \"\"\"\n","    Tokenizes English text from a string into a list of strings\n","    \"\"\"\n","    return [tok.text for tok in spacy_en.tokenizer(text)]"],"metadata":{"id":"b_NgIaop-ltM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["torchtext's Fields handle how data should be processed. All of the possible arguments are detailed here.\n","\n","We set the tokenize argument to the correct tokenization function for each, with German being the SRC (source) field and English being the TRG (target) field. The field also appends the \"start of sequence\" and \"end of sequence\" tokens via the init_token and eos_token arguments, and converts all words to lowercase."],"metadata":{"id":"_fGtuyRdqOKY"}},{"cell_type":"code","source":["SRC = Field(tokenize = tokenize_de,\n","            init_token = '<sos>',\n","            eos_token = '<eos>',\n","            lower = True,\n","            batch_first = True)\n","\n","TRG = Field(tokenize = tokenize_en,\n","            init_token = '<sos>',\n","            eos_token = '<eos>',\n","            lower = True,\n","            batch_first = True)"],"metadata":{"id":"0bQXTAf6_X85"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we download and load the train, validation and test data.\n","\n","The dataset we'll be using is the Multi30k dataset. This is a dataset with ~30,000 parallel English, German and French sentences, each with ~12 words per sentence.\n","\n","exts specifies which languages to use as the source and target (source goes first) and fields specifies which field to use for the source and target."],"metadata":{"id":"sbIVPWVuqzlj"}},{"cell_type":"code","source":["train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n","                                                    fields = (SRC, TRG))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rq61jjP__mB7","executionInfo":{"status":"ok","timestamp":1686907854025,"user_tz":-420,"elapsed":17443,"user":{"displayName":"SƠN TRƯƠNG HÀ","userId":"08876576014997023636"}},"outputId":"c5d640cb-e80c-4bee-d0dc-3ed5c5f0b691"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["downloading training.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["training.tar.gz: 100%|██████████| 1.21M/1.21M [00:02<00:00, 437kB/s]\n"]},{"output_type":"stream","name":"stdout","text":["downloading validation.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 112kB/s] \n"]},{"output_type":"stream","name":"stdout","text":["downloading mmt_task1_test2016.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 103kB/s]\n"]}]},{"cell_type":"code","source":["print(f\"Number of training examples: {len(train_data.examples)}\")\n","print(f\"Number of validation examples: {len(valid_data.examples)}\")\n","print(f\"Number of testing examples: {len(test_data.examples)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vmqocTD7_qvO","executionInfo":{"status":"ok","timestamp":1686907854026,"user_tz":-420,"elapsed":22,"user":{"displayName":"SƠN TRƯƠNG HÀ","userId":"08876576014997023636"}},"outputId":"3ec7859a-abb4-41cd-f207-6c155157d057"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training examples: 29000\n","Number of validation examples: 1014\n","Number of testing examples: 1000\n"]}]},{"cell_type":"code","source":["print(vars(train_data.examples[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V2MtutCu__FX","executionInfo":{"status":"ok","timestamp":1686907854026,"user_tz":-420,"elapsed":20,"user":{"displayName":"SƠN TRƯƠNG HÀ","userId":"08876576014997023636"}},"outputId":"3da77274-1ad7-4549-fb47-7c5f947f4735"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'src': ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"]}]},{"cell_type":"markdown","source":["Next, we'll build the vocabulary for the source and target languages. The vocabulary is used to associate each unique token with an index (an integer). The vocabularies of the source and target languages are distinct.\n","\n","Using the min_freq argument, we only allow tokens that appear at least 2 times to appear in our vocabulary. Tokens that appear only once are converted into an <unk> (unknown) token.\n","\n","It is important to note that our vocabulary should only be built from the training set and not the validation/test set. This prevents \"information leakage\" into our model, giving us artifically inflated validation/test scores."],"metadata":{"id":"lv7w1u8Dq_Jf"}},{"cell_type":"code","source":["SRC.build_vocab(train_data, min_freq = 2)\n","TRG.build_vocab(train_data, min_freq = 2)"],"metadata":{"id":"VpljSJDOAZ9w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n","print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xohE9AARAbeo","executionInfo":{"status":"ok","timestamp":1686907854027,"user_tz":-420,"elapsed":18,"user":{"displayName":"SƠN TRƯƠNG HÀ","userId":"08876576014997023636"}},"outputId":"bb5fc1c7-f7c6-4538-b226-083400991dc8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique tokens in source (de) vocabulary: 7853\n","Unique tokens in target (en) vocabulary: 5893\n"]}]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"9jaD-gl6AdSh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The final step of preparing the data is to create the iterators. These can be iterated on to return a batch of data which will have a src attribute (the PyTorch tensors containing a batch of numericalized source sentences) and a trg attribute (the PyTorch tensors containing a batch of numericalized target sentences). Numericalized is just a fancy way of saying they have been converted from a sequence of readable tokens to a sequence of corresponding indexes, using the vocabulary.\n","\n","We also need to define a torch.device. This is used to tell torchText to put the tensors on the GPU or not. We use the torch.cuda.is_available() function, which will return True if a GPU is detected on our computer. We pass this device to the iterator.\n","\n","When we get a batch of examples using an iterator we need to make sure that all of the source sentences are padded to the same length, the same with the target sentences. Luckily, torchText iterators handle this for us!\n","\n","We use a BucketIterator instead of the standard Iterator as it creates batches in such a way that it minimizes the amount of padding in both the source and target sentences."],"metadata":{"id":"kdc58ZCOrEaY"}},{"cell_type":"code","source":["BATCH_SIZE = 128\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, valid_data, test_data),\n","    batch_size = BATCH_SIZE,\n","    device = device)"],"metadata":{"id":"ebp2H8f7ArPr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttentionLayer(nn.Module):\n","    def __init__(self, hid_dim, n_heads, dropout, device):\n","        super().__init__()\n","\n","        assert hid_dim % n_heads == 0\n","\n","        self.hid_dim = hid_dim\n","        self.n_heads = n_heads\n","        self.head_dim = hid_dim // n_heads\n","\n","        self.fc_q = nn.Linear(hid_dim, hid_dim)\n","        self.fc_k = nn.Linear(hid_dim, hid_dim)\n","        self.fc_v = nn.Linear(hid_dim, hid_dim)\n","\n","        self.fc_o = nn.Linear(hid_dim, hid_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n","\n","    def forward(self, query, key, value, mask = None):\n","\n","        batch_size = query.shape[0]\n","\n","        #query = [batch size, query len, hid dim]\n","        #key = [batch size, key len, hid dim]\n","        #value = [batch size, value len, hid dim]\n","\n","        Q = self.fc_q(query)\n","        K = self.fc_k(key)\n","        V = self.fc_v(value)\n","\n","        #Q = [batch size, query len, hid dim]\n","        #K = [batch size, key len, hid dim]\n","        #V = [batch size, value len, hid dim]\n","\n","        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","\n","        #Q = [batch size, n heads, query len, head dim]\n","        #K = [batch size, n heads, key len, head dim]\n","        #V = [batch size, n heads, value len, head dim]\n","\n","        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n","\n","        #energy = [batch size, n heads, query len, key len]\n","\n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, -1e10)\n","\n","        attention = torch.softmax(energy, dim = -1)\n","\n","        #attention = [batch size, n heads, query len, key len]\n","\n","        x = torch.matmul(self.dropout(attention), V)\n","\n","        #x = [batch size, n heads, query len, head dim]\n","\n","        x = x.permute(0, 2, 1, 3).contiguous()\n","\n","        #x = [batch size, query len, n heads, head dim]\n","\n","        x = x.view(batch_size, -1, self.hid_dim)\n","\n","        #x = [batch size, query len, hid dim]\n","\n","        x = self.fc_o(x)\n","\n","        #x = [batch size, query len, hid dim]\n","\n","        return x, attention"],"metadata":{"id":"oBnqJhZ7xY46"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PositionwiseFeedforwardLayer(nn.Module):\n","    def __init__(self, hid_dim, pf_dim, dropout):\n","        super().__init__()\n","\n","        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n","        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","\n","        #x = [batch size, seq len, hid dim]\n","\n","        x = self.dropout(torch.relu(self.fc_1(x)))\n","\n","        #x = [batch size, seq len, pf dim]\n","\n","        x = self.fc_2(x)\n","\n","        #x = [batch size, seq len, hid dim]\n","\n","        return x"],"metadata":{"id":"RnTGMH10xapT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EncoderLayer(nn.Module):\n","    def __init__(self,\n","                 hid_dim,\n","                 n_heads,\n","                 pf_dim,\n","                 dropout,\n","                 device):\n","        super().__init__()\n","\n","        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n","        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n","        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim,\n","                                                                     pf_dim,\n","                                                                     dropout)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, src, src_mask):\n","\n","        #src = [batch size, src len, hid dim]\n","        #src_mask = [batch size, 1, 1, src len]\n","\n","        #self attention\n","        _src, _ = self.self_attention(src, src, src, src_mask)\n","\n","        #dropout, residual connection and layer norm\n","        src = self.self_attn_layer_norm(src + self.dropout(_src))\n","\n","        #src = [batch size, src len, hid dim]\n","\n","        #positionwise feedforward\n","        _src = self.positionwise_feedforward(src)\n","\n","        #dropout, residual and layer norm\n","        src = self.ff_layer_norm(src + self.dropout(_src))\n","\n","        #src = [batch size, src len, hid dim]\n","\n","        return src"],"metadata":{"id":"YlgmWs5Uw4FW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self,\n","                 input_dim,\n","                 hid_dim,\n","                 n_layers,\n","                 n_heads,\n","                 pf_dim,\n","                 dropout,\n","                 device,\n","                 max_length = 100):\n","        super().__init__()\n","\n","        self.device = device\n","\n","        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n","        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n","\n","        self.layers = nn.ModuleList([EncoderLayer(hid_dim,\n","                                                  n_heads,\n","                                                  pf_dim,\n","                                                  dropout,\n","                                                  device)\n","                                     for _ in range(n_layers)])\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n","\n","    def forward(self, src, src_mask):\n","\n","        #src = [batch size, src len]\n","        #src_mask = [batch size, 1, 1, src len]\n","\n","        batch_size = src.shape[0]\n","        src_len = src.shape[1]\n","\n","        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","\n","        #pos = [batch size, src len]\n","\n","        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n","\n","        #src = [batch size, src len, hid dim]\n","\n","        for layer in self.layers:\n","            src = layer(src, src_mask)\n","\n","        #src = [batch size, src len, hid dim]  30, 100, 256\n","\n","        return src"],"metadata":{"id":"jz7Xk4dCvGQy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DecoderLayer(nn.Module):\n","    def __init__(self,\n","                 hid_dim,\n","                 n_heads,\n","                 pf_dim,\n","                 dropout,\n","                 device):\n","        super().__init__()\n","\n","        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n","        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n","        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n","        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim,\n","                                                                     pf_dim,\n","                                                                     dropout)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, trg, enc_src, trg_mask, src_mask):\n","\n","        #trg = [batch size, trg len, hid dim]\n","        #enc_src = [batch size, src len, hid dim]\n","        #trg_mask = [batch size, 1, trg len, trg len]\n","        #src_mask = [batch size, 1, 1, src len]\n","\n","        #self attention\n","        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n","\n","        #dropout, residual connection and layer norm\n","        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n","\n","        #trg = [batch size, trg len, hid dim]\n","\n","        #encoder attention\n","        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n","\n","        #dropout, residual connection and layer norm\n","        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n","\n","        #trg = [batch size, trg len, hid dim]\n","\n","        #positionwise feedforward\n","        _trg = self.positionwise_feedforward(trg)\n","\n","        #dropout, residual and layer norm\n","        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n","\n","        #trg = [batch size, trg len, hid dim]\n","        #attention = [batch size, n heads, trg len, src len]\n","\n","        return trg, attention"],"metadata":{"id":"_zGBY_nZcbUa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self,\n","                 output_dim,\n","                 hid_dim,\n","                 n_layers,\n","                 n_heads,\n","                 pf_dim,\n","                 dropout,\n","                 device,\n","                 max_length = 100):\n","        super().__init__()\n","\n","        self.device = device\n","\n","        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n","        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n","\n","        self.layers = nn.ModuleList([DecoderLayer(hid_dim,\n","                                                  n_heads,\n","                                                  pf_dim,\n","                                                  dropout,\n","                                                  device)\n","                                     for _ in range(n_layers)])\n","\n","        self.fc_out = nn.Linear(hid_dim, output_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n","\n","    def forward(self, trg, enc_src, trg_mask, src_mask):\n","\n","        #trg = [batch size, trg len]\n","        #enc_src = [batch size, src len, hid dim]\n","        #trg_mask = [batch size, 1, trg len, trg len]\n","        #src_mask = [batch size, 1, 1, src len]\n","\n","        batch_size = trg.shape[0]\n","        trg_len = trg.shape[1]\n","\n","        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","\n","        #pos = [batch size, trg len]\n","\n","        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n","\n","        #trg = [batch size, trg len, hid dim]\n","\n","        for layer in self.layers:\n","            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n","\n","        #trg = [batch size, trg len, hid dim]\n","        #attention = [batch size, n heads, trg len, src len]\n","\n","        output = self.fc_out(trg)\n","\n","        #output = [batch size, trg len, output dim]\n","\n","        return output, attention"],"metadata":{"id":"ow89BOKfcZt1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Seq2Seq(nn.Module):\n","    def __init__(self,\n","                 encoder,\n","                 decoder,\n","                 src_pad_idx,\n","                 trg_pad_idx,\n","                 device):\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_pad_idx = src_pad_idx\n","        self.trg_pad_idx = trg_pad_idx\n","        self.device = device\n","\n","    def make_src_mask(self, src):\n","\n","        #src = [batch size, src len]\n","\n","        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","\n","        #src_mask = [batch size, 1, 1, src len]\n","\n","        return src_mask\n","\n","    def make_trg_mask(self, trg):\n","\n","        #trg = [batch size, trg len]\n","\n","        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n","\n","        #trg_pad_mask = [batch size, 1, 1, trg len]\n","\n","        trg_len = trg.shape[1]\n","\n","        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n","\n","        #trg_sub_mask = [trg len, trg len]\n","\n","        trg_mask = trg_pad_mask & trg_sub_mask\n","\n","        #trg_mask = [batch size, 1, trg len, trg len]\n","\n","        return trg_mask\n","\n","    def forward(self, src, trg):\n","\n","        #src = [batch size, src len]\n","        #trg = [batch size, trg len]\n","\n","        src_mask = self.make_src_mask(src)\n","        trg_mask = self.make_trg_mask(trg)\n","\n","        #src_mask = [batch size, 1, 1, src len]\n","        #trg_mask = [batch size, 1, trg len, trg len]\n","\n","        enc_src = self.encoder(src, src_mask)\n","\n","        #enc_src = [batch size, src len, hid dim]\n","\n","        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n","\n","        #output = [batch size, trg len, output dim]\n","        #attention = [batch size, n heads, trg len, src len]\n","\n","        return output, attention"],"metadata":{"id":"8b5gxxN0cfZN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["INPUT_DIM = len(SRC.vocab)\n","OUTPUT_DIM = len(TRG.vocab)\n","HID_DIM = 256\n","ENC_LAYERS = 3\n","DEC_LAYERS = 3\n","ENC_HEADS = 8\n","DEC_HEADS = 8\n","ENC_PF_DIM = 512\n","DEC_PF_DIM = 512\n","ENC_DROPOUT = 0.1\n","DEC_DROPOUT = 0.1\n","\n","enc = Encoder(INPUT_DIM,\n","              HID_DIM,\n","              ENC_LAYERS,\n","              ENC_HEADS,\n","              ENC_PF_DIM,\n","              ENC_DROPOUT,\n","              device)\n","\n","dec = Decoder(OUTPUT_DIM,\n","              HID_DIM,\n","              DEC_LAYERS,\n","              DEC_HEADS,\n","              DEC_PF_DIM,\n","              DEC_DROPOUT,\n","              device)"],"metadata":{"id":"Tu4TQDkrjbVW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n","TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n","\n","model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"],"metadata":{"id":"8NUc8Ub4jdKB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9SPVaBI4j1Y8","executionInfo":{"status":"ok","timestamp":1686910411684,"user_tz":-420,"elapsed":13,"user":{"displayName":"SƠN TRƯƠNG HÀ","userId":"08876576014997023636"}},"outputId":"7523f74e-0b6c-4f1b-8092-1fc448c13428"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 9,038,341 trainable parameters\n"]}]},{"cell_type":"code","source":["def initialize_weights(m):\n","    if hasattr(m, 'weight') and m.weight.dim() > 1:\n","        nn.init.xavier_uniform_(m.weight.data)\n","\n","model.apply(initialize_weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ee9GWObWj47f","executionInfo":{"status":"ok","timestamp":1686910411684,"user_tz":-420,"elapsed":11,"user":{"displayName":"SƠN TRƯƠNG HÀ","userId":"08876576014997023636"}},"outputId":"9e6f1797-e4ef-4834-fe2e-629841456abd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (tok_embedding): Embedding(7853, 256)\n","    (pos_embedding): Embedding(100, 256)\n","    (layers): ModuleList(\n","      (0-2): 3 x EncoderLayer(\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (self_attention): MultiHeadAttentionLayer(\n","          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n","          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n","          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n","          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n","          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n","          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (tok_embedding): Embedding(5893, 256)\n","    (pos_embedding): Embedding(100, 256)\n","    (layers): ModuleList(\n","      (0-2): 3 x DecoderLayer(\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (self_attention): MultiHeadAttentionLayer(\n","          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n","          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n","          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n","          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (encoder_attention): MultiHeadAttentionLayer(\n","          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n","          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n","          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n","          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n","          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n","          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (fc_out): Linear(in_features=256, out_features=5893, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n",")"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["LEARNING_RATE = 0.0005\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"],"metadata":{"id":"V7K4tqrsj7-J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"],"metadata":{"id":"i_akfNu7j_7-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, iterator, optimizer, criterion, clip):\n","\n","    model.train()\n","\n","    epoch_loss = 0\n","\n","    for i, batch in enumerate(iterator):\n","\n","        src = batch.src\n","        trg = batch.trg\n","\n","        optimizer.zero_grad()\n","\n","        output, _ = model(src, trg[:,:-1])\n","\n","        #output = [batch size, trg len - 1, output dim]\n","        #trg = [batch size, trg len]\n","\n","        output_dim = output.shape[-1]\n","\n","        output = output.contiguous().view(-1, output_dim)\n","        trg = trg[:,1:].contiguous().view(-1)\n","\n","        #output = [batch size * trg len - 1, output dim]\n","        #trg = [batch size * trg len - 1]\n","\n","        loss = criterion(output, trg)\n","\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)"],"metadata":{"id":"j7W4m0zWkBqC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"metadata":{"id":"-qVw7FWQmQPW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, iterator, criterion):\n","\n","    model.eval()\n","\n","    epoch_loss = 0\n","\n","    with torch.no_grad():\n","\n","        for i, batch in enumerate(iterator):\n","\n","            src = batch.src\n","            trg = batch.trg\n","\n","            output, _ = model(src, trg[:,:-1])\n","\n","            #output = [batch size, trg len - 1, output dim]\n","            #trg = [batch size, trg len]\n","\n","            output_dim = output.shape[-1]\n","\n","            output = output.contiguous().view(-1, output_dim)\n","            trg = trg[:,1:].contiguous().view(-1)\n","\n","            #output = [batch size * trg len - 1, output dim]\n","            #trg = [batch size * trg len - 1]\n","\n","            loss = criterion(output, trg)\n","\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)"],"metadata":{"id":"j2MfbsN-mUA-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["N_EPOCHS = 10\n","CLIP = 1\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","\n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n","    valid_loss = evaluate(model, valid_iterator, criterion)\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut6-model.pt')\n","\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OZZQngGZmZAK","executionInfo":{"status":"ok","timestamp":1686910569938,"user_tz":-420,"elapsed":158261,"user":{"displayName":"SƠN TRƯƠNG HÀ","userId":"08876576014997023636"}},"outputId":"0db9beef-1936-47b3-feb1-9f20c65e6aaf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Time: 0m 15s\n","\tTrain Loss: 4.227 | Train PPL:  68.522\n","\t Val. Loss: 3.005 |  Val. PPL:  20.195\n","Epoch: 02 | Time: 0m 15s\n","\tTrain Loss: 2.802 | Train PPL:  16.473\n","\t Val. Loss: 2.289 |  Val. PPL:   9.863\n","Epoch: 03 | Time: 0m 16s\n","\tTrain Loss: 2.231 | Train PPL:   9.305\n","\t Val. Loss: 1.980 |  Val. PPL:   7.245\n","Epoch: 04 | Time: 0m 15s\n","\tTrain Loss: 1.879 | Train PPL:   6.550\n","\t Val. Loss: 1.806 |  Val. PPL:   6.089\n","Epoch: 05 | Time: 0m 15s\n","\tTrain Loss: 1.635 | Train PPL:   5.128\n","\t Val. Loss: 1.728 |  Val. PPL:   5.631\n","Epoch: 06 | Time: 0m 15s\n","\tTrain Loss: 1.444 | Train PPL:   4.238\n","\t Val. Loss: 1.668 |  Val. PPL:   5.303\n","Epoch: 07 | Time: 0m 15s\n","\tTrain Loss: 1.295 | Train PPL:   3.650\n","\t Val. Loss: 1.639 |  Val. PPL:   5.150\n","Epoch: 08 | Time: 0m 15s\n","\tTrain Loss: 1.167 | Train PPL:   3.212\n","\t Val. Loss: 1.626 |  Val. PPL:   5.083\n","Epoch: 09 | Time: 0m 15s\n","\tTrain Loss: 1.060 | Train PPL:   2.886\n","\t Val. Loss: 1.642 |  Val. PPL:   5.164\n","Epoch: 10 | Time: 0m 15s\n","\tTrain Loss: 0.965 | Train PPL:   2.625\n","\t Val. Loss: 1.646 |  Val. PPL:   5.184\n"]}]},{"cell_type":"code","source":["model.load_state_dict(torch.load('tut6-model.pt'))\n","\n","test_loss = evaluate(model, test_iterator, criterion)\n","\n","print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QOfMVB5Cm4HI","executionInfo":{"status":"ok","timestamp":1686910569939,"user_tz":-420,"elapsed":19,"user":{"displayName":"SƠN TRƯƠNG HÀ","userId":"08876576014997023636"}},"outputId":"6c85c16e-b16d-4c97-e610-d0772e7666a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["| Test Loss: 1.672 | Test PPL:   5.325 |\n"]}]},{"cell_type":"code","source":["def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n","\n","    model.eval()\n","\n","    if isinstance(sentence, str):\n","        nlp = spacy.load('de_core_news_sm')\n","        tokens = [token.text.lower() for token in nlp(sentence)]\n","    else:\n","        tokens = [token.lower() for token in sentence]\n","\n","    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n","\n","    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n","\n","    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n","\n","    src_mask = model.make_src_mask(src_tensor)\n","\n","    with torch.no_grad():\n","        enc_src = model.encoder(src_tensor, src_mask)\n","\n","    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n","\n","    for i in range(max_len):\n","\n","        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n","\n","        trg_mask = model.make_trg_mask(trg_tensor)\n","\n","        with torch.no_grad():\n","            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n","\n","        pred_token = output.argmax(2)[:,-1].item()\n","\n","        trg_indexes.append(pred_token)\n","\n","        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n","            break\n","\n","    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n","\n","    return trg_tokens[1:], attention"],"metadata":{"id":"P319IT8-m4qX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["src = ['eine', 'mutter', 'und', 'ihr', 'kleiner', 'sohn', 'genießen', 'einen', 'schönen', 'tag', 'im', 'freien', '.']\n","translation, attention = translate_sentence(src, SRC, TRG, model, device)\n","\n","print(f'predicted trg = {translation}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x0SnCqoDnNeA","executionInfo":{"status":"ok","timestamp":1686910655618,"user_tz":-420,"elapsed":518,"user":{"displayName":"SƠN TRƯƠNG HÀ","userId":"08876576014997023636"}},"outputId":"5e53b1d3-b052-4658-fc26-7f2fb4ee43cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["predicted trg = ['a', 'mother', 'and', 'her', 'baby', 'enjoying', 'a', 'nice', 'day', 'outside', '.', '<eos>']\n"]}]}]}